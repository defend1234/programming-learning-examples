{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some IO operations in Numpy for loading and saving matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## npy file usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "(15000, 3)\n"
     ]
    }
   ],
   "source": [
    "def load_binary_data():\n",
    "    file_path = \"/home/zhanghm/Research/V100/FaceFormer/data/big_mouth_mask.npy\"\n",
    "    data = np.load(file_path)\n",
    "    print(data.shape, data.min(), data.max())\n",
    "\n",
    "    print(np.count_nonzero(data))\n",
    "\n",
    "\n",
    "def load_dict_npy():\n",
    "    \"\"\"The npy file could be used to save dictionary data\n",
    "    \"\"\"\n",
    "    npy_fp = \"/home/zhanghm/Research/PU/ShapeFormer/experiments/vqdif/my_shapenet_res16/results/VisSparseRecon3D/computed/1.npy\"\n",
    "    data = np.load(npy_fp, allow_pickle=True)\n",
    "    content = data.item()\n",
    "\n",
    "    print(type(content), content.keys())\n",
    "    for key, value in content.items():\n",
    "        try:\n",
    "            print(key, value.shape)\n",
    "        except:\n",
    "            print(key, type(value))\n",
    "            print(value.keys())\n",
    "\n",
    "    print(content.keys(), content['logits'].shape, content['quant_ind'].shape)\n",
    "    print(np.unique(content['quant_ind']))\n",
    "\n",
    "def load_FaceFormer_data():\n",
    "    npy_fp = \"/home/zhanghm/Research/TalkingFace/FaceFormer/vocaset/vertices_npy/FaceTalk_170725_00137_TA_sentence01.npy\"\n",
    "    data = np.load(npy_fp)\n",
    "    print(data.shape)\n",
    "\n",
    "def load_ShapeNet55():\n",
    "    input_fp = \"/home/zhanghm/Research/PointCloud/Point-MAE/data/ShapeNet55-34/shapenet_pc/02691156-1a04e3eab45ca15dd86060f189eb133.npy\"\n",
    "    data = np.load(input_fp).astype(np.float32)\n",
    "    print(data.shape)\n",
    "\n",
    "    np.savetxt(\"shapenet.xyz\", data)\n",
    "\n",
    "def load_ShapeNetPC15k():\n",
    "    input_fp = \"/data1/zhanghm/Datasets/ShapeNet/ShapeNetCore.v2.PC15k/02691156/train/47c7b3cb099b3212d1c83bc8b134e4d8.npy\"\n",
    "    data = np.load(input_fp)\n",
    "    print(data.dtype)\n",
    "    print(data.shape)\n",
    "\n",
    "    np.savetxt(\"shapenet_15k.xyz\", data)\n",
    "\n",
    "\n",
    "load_ShapeNetPC15k()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(402, 257) (402, 257)\n",
      "-0.2383115 0.2779872\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr1 = np.load(\"/home/zhanghm/Research/StyleGAN/Deep3DFaceRecon_pytorch/pred_coeffs_new.npy\")\n",
    "arr2 = np.load(\"/home/zhanghm/Research/StyleGAN/Deep3DFaceRecon_pytorch/pred_coeffs.npy\")\n",
    "print(arr1.shape, arr2.shape)\n",
    "\n",
    "diff = arr1 - arr2\n",
    "print(diff.min(), diff.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## npz file usages\n",
    "https://vimsky.com/examples/usage/python-numpy.savez.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.lib.npyio.NpzFile'>\n",
      "['images', 'cam_poses', 'data']\n",
      "(50, 200, 200, 3) 0.011765 1.0\n",
      "(2048, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def read_points2nerf_npz():\n",
    "    fp = \"50_fff64da26715b520e40201fea1ad0f1.npz\"\n",
    "    data = np.load(fp)\n",
    "    print(type(data))\n",
    "    print(data.files)\n",
    "\n",
    "    images = data['images']\n",
    "    print(images.shape, images.min(), images.max())\n",
    "    \n",
    "    img = Image.fromarray((images[0] * 255.0).astype(np.uint8))\n",
    "    img.save(\"dd.png\")\n",
    "\n",
    "    points = data['data']\n",
    "    print(points.shape)\n",
    "    np.savetxt(\"dd.xyz\", points[:, :3])\n",
    "\n",
    "\n",
    "read_points2nerf_npz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['points', 'normals']\n",
      "(50000, 3) -0.5075068 0.50887996\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_npz():\n",
    "    npz_file_path = \"./data/obama2.npz\"\n",
    "    netparams = np.load(open(npz_file_path, 'rb'))\n",
    "\n",
    "    print(netparams.files)  ## Get the dictionary keys\n",
    "\n",
    "    content = netparams['face']\n",
    "    print(type(netparams), content.shape, np.min(content), np.max(content))\n",
    "\n",
    "    ## Another npz file\n",
    "    real_params = \"/home/haimingzhang/Research/Face/FACIAL/gangqiang_2_preprocess/deep3dface.npz\"\n",
    "    realparams = np.load(open(real_params, 'rb'))['face']\n",
    "    print(realparams.shape)\n",
    "\n",
    "\n",
    "def load_npz2():\n",
    "    npz_fp = \"/home/zhanghm/Research/PU/ShapeFormer/experiments/my_demo_vqdif/eval/0.npz\"\n",
    "    content = np.load(open(npz_fp, \"rb\"))\n",
    "    print(content.files)\n",
    "\n",
    "    eval_pc = content['eval_pc']\n",
    "    print(eval_pc.shape)\n",
    "    np.savetxt(\"eval_pc.xyz\", eval_pc)\n",
    "\n",
    "def load_npz3():\n",
    "    npz_fp = \"/home/zhanghm/Research/PU/Github/sapcu/data/ShapeNet/02828884/faa74f8980fadf504777535b9098089a/pointcloud.npz\"\n",
    "    npz_fp = \"/home/zhanghm/Research/PU/Github/sapcu/data/ShapeNet/02828884/f5557538f4c6d755d295b24579cf55b8/pointcloud.npz\"\n",
    "    npz_fp = \"/home/zhanghm/Research/PU/Github/sapcu/data/ShapeNet/02933112/1a9a91aa5e3306ec5938fc2058ab2dbe/pointcloud.npz\"\n",
    "    npz_fp = \"/home/zhanghm/Research/PU/Github/sapcu/data/ShapeNet/02933112/1d898bbd8bbad8f98430b7446f9e1252/pointcloud.npz\"\n",
    "    content = np.load(npz_fp)\n",
    "    print(content.files)\n",
    "\n",
    "    points = content['points']\n",
    "    print(points.shape, points.min(), points.max())\n",
    "    np.savetxt(\"points.xyz\", points)\n",
    "\n",
    "def load_general_npz():\n",
    "    npz_fp = \"/home/zhanghm/Research/PU/Github/OnSurfacePrior/data/input.ply.npz\"\n",
    "    content = np.load(npz_fp)\n",
    "    print(content.files)\n",
    "    for key, value in content.items():\n",
    "        print(key, value.shape, value.min(), value.max())\n",
    "        if key == \"sample\" or key == \"pointcloud_s\":\n",
    "            np.savetxt(f\"{key}.xyz\", value)\n",
    "\n",
    "\n",
    "def load_NeuralPull_data():\n",
    "    npz_fp = \"/home/zhanghm/Research/PU/Github/NeuralPull-Pytorch/data/gargoyle.npz\"\n",
    "    # npz_fp = \"/home/zhanghm/Research/PU/Github/NeuralPull-Pytorch/data/punet_sample_100.npz\"\n",
    "    # npz_fp = \"/home/zhanghm/Research/PU/CAP-UDF/data/owndata/query_data/punet_sample_100.npz\"\n",
    "    \n",
    "    load_data = np.load(npz_fp)\n",
    "    print(load_data.files)\n",
    "\n",
    "    print(load_data['sample_near'].shape, load_data['sample'].shape, load_data['point'].shape)\n",
    "    np.savetxt(\"0_sample_near.xyz\", load_data['sample_near'][0])\n",
    "    np.savetxt(\"0_sample.xyz\", load_data['sample'][0])\n",
    "    np.savetxt(\"0_point.xyz\", load_data['point'])\n",
    "\n",
    "\n",
    "def load_CAP_UDF_npz():\n",
    "    npz_fp = \"/home/zhanghm/Research/PU/CAP-UDF/data/shapenetCars2/query_data/3e5e4ff60c151baee9d84a67fdc5736.npz\"\n",
    "    # npz_fp = \"/home/zhanghm/Research/PU/CAP-UDF/data/shapenetCars/query_data/3e5e4ff60c151baee9d84a67fdc5736.npz\"\n",
    "    \n",
    "    load_data = np.load(npz_fp)\n",
    "    print(load_data.files)\n",
    "    sample = load_data['sample']\n",
    "    point = load_data['point']\n",
    "    sample_near = load_data['sample_near']\n",
    "    print(sample.shape, point.shape, sample_near.shape)\n",
    "\n",
    "    np.savetxt(\"sample_near.xyz\", sample_near[0])\n",
    "    np.savetxt(\"sample.xyz\", sample[0])\n",
    "    np.savetxt(\"gt.xyz\", point)\n",
    "\n",
    "\n",
    "def write_npz():\n",
    "    face_params = np.zeros((10, 64), np.float)\n",
    "    np.savez(\"./data/example.npz\", face=face_params)\n",
    "    \n",
    "load_npz3()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load .txt/.xyz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 3) float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_xyz_pc_data():\n",
    "    xyz_fp = \"/home/zhanghm/Research/Programming/cv-fighter/GenPU_Dataset2/Bed/e0bd1850ff5dd7e9485059ffe7a2f9c7/gt_pc.xyz\"\n",
    "    xyz_fp = \"/data/zhanghm/Datasets/sapcu/gt/camel.xyz\"\n",
    "    input = np.loadtxt(xyz_fp)\n",
    "    print(input.shape, input.dtype)\n",
    "\n",
    "\n",
    "def load_CAP_UDF_data():\n",
    "    xyz_fp = \"/home/zhanghm/Research/PU/CAP-UDF/data/shapenetCars/input/3e5e4ff60c151baee9d84a67fdc5736.xyz\"\n",
    "    xyz_fp = \"/home/zhanghm/Research/PU/CAP-UDF/data/shapenetCars/input/c153fa692b6b6dd7ab7ec554b18515bb.xyz\"\n",
    "    input = np.loadtxt(xyz_fp)\n",
    "    print(input.shape, input.dtype, input.min(), input.max())\n",
    "\n",
    "\n",
    "load_xyz_pc_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.00029898e-01 -4.05742265e-02 -4.33935225e-01  5.64115703e-01]\n",
      " [ 4.35828000e-01 -8.37893337e-02 -8.96121204e-01  1.16495752e+00]\n",
      " [ 2.94297905e-07 -9.95657086e-01  9.30962339e-02 -1.21024936e-01]\n",
      " [-0.00000000e+00  0.00000000e+00 -0.00000000e+00  1.00000000e+00]]\n",
      "1.299999843434892\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "fp = \"/data1/zhanghm/Code/NeRF/vision-nerf/dataset/srn_chairs/chairs_train/chairs_2.0_train/1a74a83fa6d24b3cacd67ce2c72c02e/pose/000003.txt\"\n",
    "fp = \"/data1/zhanghm/Code/NeRF/vision-nerf/dataset/srn_cars/cars_train/1a4ef4a2a639f172f13d1237e1429e9e/pose/000009.txt\"\n",
    "pose = np.loadtxt(fp).reshape(4, 4)\n",
    "print(pose)\n",
    "\n",
    "radius = np.linalg.norm(pose[:3, 3])\n",
    "print(radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine different parameters from two persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (1419, 257) -3.4403297901153564 3.5975096225738525\n"
     ]
    }
   ],
   "source": [
    "another_deep3dface_npz = \"/home/haimingzhang/Research/Face/FACIAL/video_preprocessed/id00001/gangqiang_3/deep3dface.npz\"\n",
    "\n",
    "netparams = np.load(open(another_deep3dface_npz, 'rb'))['face']\n",
    "\n",
    "print(type(netparams), netparams.shape, np.min(netparams), np.max(netparams))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(204, 15069) -0.18839313089847565 0.13654573261737823\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/haimingzhang/Research/Face/voca/training_data/init_expression_basis.npy\"\n",
    "file_path = \"/home/zhanghm/Research/Github/FaceFormer/vocaset/vertices_npy/FaceTalk_170725_00137_TA_sentence01.npy\"\n",
    "\n",
    "expression_basis = np.load(file_path)\n",
    "print(expression_basis.shape, np.min(expression_basis), np.max(expression_basis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uint8\n",
      "0.0019309520721435547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "time1 = time.time()\n",
    "mask = np.load(\"/home/haimingzhang/Research/Programming/cv-fighter/face_fighter/3DMM/mask.npy\")\n",
    "# mask = cv2.imread(\"/home/haimingzhang/Research/Programming/cv-fighter/face_fighter/3DMM/mask.png\")\n",
    "# mask = np.load(open(\"/home/haimingzhang/Research/Programming/cv-fighter/face_fighter/3DMM/mask.npz\", \"rb\"))['mask']\n",
    "print(mask.dtype)\n",
    "time2 = time.time()\n",
    "print(time2 - time1)\n",
    "\n",
    "def get_contour(im):\n",
    "    contours, hierarchy = cv2.findContours(im.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    out = np.zeros_like(im)\n",
    "    out = cv2.fillPoly(out, contours, 255)\n",
    "\n",
    "    return out\n",
    "\n",
    "mask2 = get_contour(mask)\n",
    "cv2.imwrite(\"mask2.png\", mask2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38-torch100-cu11')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "56d0891f27fe2db3830dc2a05bc70f05135b0c30ed7c190a150d1aca2da3af60"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
